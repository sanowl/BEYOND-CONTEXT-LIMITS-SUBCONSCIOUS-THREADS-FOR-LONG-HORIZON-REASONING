# TIM (Thread Inference Model) Implementation Requirements
# Based on "Beyond Context Limits: Subconscious Threads for Long-Horizon Reasoning"

# Core dependencies
pydantic>=2.0.0
numpy>=1.21.0

# JSON schema validation
jsonschema>=4.0.0

# HTTP client for tool servers
aiohttp>=3.8.0
requests>=2.28.0

# Note: sqlite3 is built-in with Python (no separate install needed)

# Optional dependencies for production TIM implementation
# Uncomment based on your needs:

# For Transformers backend (real TIM implementation):
# torch>=2.0.0
# transformers>=4.35.0
# accelerate>=0.20.0
# flash-attn>=2.3.0

# For vLLM backend (production inference):
# vllm>=0.2.0
# ray>=2.8.0

# For enhanced JSON processing:
# jsonpath-ng>=1.5.0

# For production deployment:
# fastapi>=0.100.0
# uvicorn>=0.22.0
# gunicorn>=20.1.0

# For monitoring and logging:
# prometheus-client>=0.16.0
# structlog>=23.0.0

# For advanced JSON processing:
# jsonpath-ng>=1.5.0
# jinja2>=3.1.0

# For testing:
# pytest>=7.0.0
# pytest-asyncio>=0.21.0
# pytest-mock>=3.10.0

# For development:
# black>=23.0.0
# isort>=5.12.0
# mypy>=1.3.0